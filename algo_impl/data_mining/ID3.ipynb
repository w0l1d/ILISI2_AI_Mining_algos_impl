{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def entropy(values):\n",
    "    result = 0.0\n",
    "    div = sum(values)\n",
    "    for v in values:\n",
    "        if v > 0:\n",
    "            result -= v/div * math.log(v/div, 2)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def entropyTotale(values):\n",
    "    result = 0.0\n",
    "    total = 0\n",
    "    for e in values:\n",
    "        total += sum(e)\n",
    "    for e in values:\n",
    "        card = sum(e)\n",
    "        result += entropy(e) * card / total\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def entropyContinu(values):\n",
    "    result = 0\n",
    "    total = 0\n",
    "    for e in values:\n",
    "        total += sum(e)\n",
    "    for e in values:\n",
    "        print(\"****\", entropy(e) * sum(e) / total)\n",
    "        result += entropy(e) * sum(e) / total\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def frequency_by_bins(df: pd.DataFrame, first_col: str, second_col: str):\n",
    "    # Compute the bin edges as the average of each pair of consecutive values in the first column\n",
    "    bin_edges = (df[first_col] + df[first_col].shift()) / 2\n",
    "\n",
    "    # Drop the first row since it will have a NaN value for the bin edge\n",
    "    bin_edges = bin_edges.dropna()\n",
    "\n",
    "    # Initialize empty dictionaries to store the frequencies before and after each bin edge\n",
    "    freqs_before = {}\n",
    "    freqs_after = {}\n",
    "\n",
    "    min_entropy = 1000\n",
    "    min_avg = 0\n",
    "    min_info = []\n",
    "    # Iterate over the bin edges and compute the frequencies before and after each one\n",
    "    for edge in bin_edges:\n",
    "        glob_list = []\n",
    "        sub_list1 = []\n",
    "        sun_list2 = []\n",
    "        print(edge)\n",
    "        # Find the index of the current bin edge in the original DataFrame\n",
    "        idx = df[first_col].searchsorted(edge)\n",
    "        #print(\"Before ==> \")\n",
    "        for v in df.iloc[:idx][second_col].value_counts():\n",
    "            #print(\"  \", v)\n",
    "            sub_list1.append(v)\n",
    "        glob_list.append(sub_list1)\n",
    "        #print(\"After ==> \")\n",
    "        for v in df.iloc[idx:][second_col].value_counts():\n",
    "            #print(\"  \", v)\n",
    "            sun_list2.append(v)\n",
    "        glob_list.append(sun_list2)\n",
    "        print(glob_list)\n",
    "\n",
    "        entr_cont = entropyContinu(glob_list)\n",
    "        print(\"entropy =====> \", entr_cont)\n",
    "        if entr_cont < min_entropy:\n",
    "            min_entropy = entr_cont\n",
    "            min_avg = edge\n",
    "\n",
    "        # Compute the frequencies of the second column before and after the current bin edge\n",
    "        freqs_before[edge] = df.iloc[:idx][second_col].value_counts()\n",
    "        freqs_after[edge] = df.iloc[idx:][second_col].value_counts()\n",
    "\n",
    "        # Combine the dictionaries into a single DataFrame and return it\n",
    "    #freqs_df = pd.concat(\n",
    "        #[pd.concat([freqs_before[k], freqs_after[k]], axis=1, keys=['before', 'after'])\n",
    "         #for k in freqs_before.keys()],\n",
    "        #axis=1\n",
    "    #)\n",
    "    #freqs_df.columns.names = ['bin', 'value']\n",
    "    print(min_avg)\n",
    "    print(min_entropy)\n",
    "    min_info.append(min_entropy)\n",
    "    min_info.append(min_avg)\n",
    "    min_info.append(first_col)\n",
    "\n",
    "    #df1 = df[df[first_col] < 57.5]\n",
    "    #df2 = df[df[first_col] >= 57.5]\n",
    "\n",
    "    # Print the resulting groups\n",
    "    #print('Group 1:\\n', df1)\n",
    "    #print('Group 2:\\n', df2)\n",
    "\n",
    "    return min_info\n",
    "\n",
    "    #print(freqs_df)\n",
    "\n",
    "    #return groups"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data.csv')\n",
    "#frequency_by_bins(data, 'Poids', 'class')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def group_by_column(df: pd.DataFrame, column_to_group: str):\n",
    "    groups = df.drop(column_to_group, axis=1).groupby(df[column_to_group])\n",
    "    for group_name, group_data in groups:\n",
    "        print(\"\\n*Group:\", group_name)\n",
    "        print(group_data)\n",
    "        id3(group_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def id3(df: pd.DataFrame, test_date: pd.DataFrame = None, ignore_cols:list=[]):\n",
    "    ignore_cols.append('class')\n",
    "    ignore_cols.append('id')\n",
    "    class_col = 'class'\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ignore_cols]\n",
    "\n",
    "    cont_attr = []\n",
    "    for col in numeric_cols:\n",
    "        #counts = df.groupby(class_col)[col].value_counts()\n",
    "        #print(f\"Value counts for column '{col}' by {class_col}:\\n{counts}\\n\")\n",
    "        print(f\"\\nNumeric column '{col}':\")\n",
    "        cont_attr = frequency_by_bins(df, col, class_col)\n",
    "    if not cont_attr:\n",
    "        cont_attr = [1000]\n",
    "\n",
    "    non_numeric_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    non_numeric_cols = [col for col in non_numeric_cols if col not in ignore_cols]\n",
    "\n",
    "    non_numeric_counts = {}\n",
    "    for col in non_numeric_cols:\n",
    "        counts = df.groupby(class_col)[col].value_counts()\n",
    "        non_numeric_counts[col] = counts\n",
    "        #print(f\"Value counts for column '{col}' by {class_col}:\\n{counts}\\n\")\n",
    "\n",
    "    non_numeric_entropys = {}\n",
    "    for col, counts in non_numeric_counts.items():\n",
    "        print(f\"\\nNon-numeric column '{col}':\")\n",
    "        entropy_list = []\n",
    "        for col_value in df[col].unique():\n",
    "            group_counts = df.groupby(col)[class_col].value_counts()\n",
    "            #print(\"*****\", group_counts)\n",
    "            try:\n",
    "                attr_list = []\n",
    "                #print(\"**********\", col_value)\n",
    "                value_counts = group_counts[col_value]\n",
    "                for v in value_counts.to_dict().values():\n",
    "                    #print(v)\n",
    "                    attr_list.append(v)\n",
    "                entropy_list.append(attr_list)\n",
    "                print(f\"  '{col_value} ==> '{entropy(attr_list)}'\")\n",
    "                #print(attr_list)\n",
    "                #print(f\"Value '{col_value}': {value_counts.to_dict()}\")\n",
    "            except KeyError:\n",
    "                print(f\"Value '{col_value}': {0}\")\n",
    "        #print(entropy_list)\n",
    "        non_numeric_entropys[col] = entropyTotale(entropy_list)\n",
    "        print(f\"column'{col}' ==> entropy : '{non_numeric_entropys[col]}'\")\n",
    "    #print(min(non_numeric_entropys, key=non_numeric_entropys.get))\n",
    "    #print(min(non_numeric_entropys.values()))\n",
    "    min_non_numeric = 1000\n",
    "    if sum(non_numeric_entropys.values()) == 0.0 and (cont_attr[0] == 0.0 or cont_attr[0] == 1000):\n",
    "        print(\"******No more split\")\n",
    "    else:\n",
    "        if sum(non_numeric_entropys.values()) > 0.0:\n",
    "            min_non_numeric = min(non_numeric_entropys.values())\n",
    "        if min_non_numeric < cont_attr[0]:\n",
    "            #print(cont_attr[0])\n",
    "            print(\"===========> split by : \", min(non_numeric_entropys, key=non_numeric_entropys.get))\n",
    "            group_by_column(df, min(non_numeric_entropys, key=non_numeric_entropys.get))\n",
    "        else:\n",
    "            print(\"===========> split by : \", cont_attr[2])\n",
    "            df1 = df[df[cont_attr[2]] < cont_attr[1]]\n",
    "            df2 = df[df[cont_attr[2]] >= cont_attr[1]]\n",
    "            print(\"\\n*Group < \", cont_attr[1])\n",
    "            print(df1)\n",
    "            id3(df1)\n",
    "            print(\"\\n*Group >= \", cont_attr[1])\n",
    "            print(df2)\n",
    "            id3(df2)\n",
    "        #group_by_column(df, min(non_numeric_entropys, key=non_numeric_entropys.get))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric column 'Poids':\n",
      "42.5\n",
      "[[1], [4, 3]]\n",
      "**** 0.0\n",
      "**** 0.8620746190299702\n",
      "entropy =====>  0.8620746190299702\n",
      "50.0\n",
      "[[2], [4, 2]]\n",
      "**** 0.0\n",
      "**** 0.6887218755408672\n",
      "entropy =====>  0.6887218755408672\n",
      "57.5\n",
      "[[3], [4, 1]]\n",
      "**** 0.0\n",
      "**** 0.4512050593046014\n",
      "entropy =====>  0.4512050593046014\n",
      "62.5\n",
      "[[4], [4]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "70.0\n",
      "[[4, 1], [3]]\n",
      "**** 0.4512050593046014\n",
      "**** 0.0\n",
      "entropy =====>  0.4512050593046014\n",
      "77.5\n",
      "[[4, 2], [2]]\n",
      "**** 0.6887218755408672\n",
      "**** 0.0\n",
      "entropy =====>  0.6887218755408672\n",
      "82.5\n",
      "[[4, 3], [1]]\n",
      "**** 0.8620746190299702\n",
      "**** 0.0\n",
      "entropy =====>  0.8620746190299702\n",
      "62.5\n",
      "0.0\n",
      "\n",
      "Non-numeric column 'Cheveux':\n",
      "  'bl ==> '0.8112781244591328'\n",
      "  'br ==> '0.9182958340544896'\n",
      "  'r ==> '0.0'\n",
      "column'Cheveux' ==> entropy : '0.75'\n",
      "\n",
      "Non-numeric column 'Taille':\n",
      "  'p ==> '0.9182958340544896'\n",
      "  'm ==> '0.9182958340544896'\n",
      "  'g ==> '1.0'\n",
      "column'Taille' ==> entropy : '0.9387218755408672'\n",
      "\n",
      "Non-numeric column 'Vegetarien':\n",
      "  'o ==> '0.0'\n",
      "  'n ==> '0.7219280948873623'\n",
      "column'Vegetarien' ==> entropy : '0.4512050593046014'\n",
      "===========> split by :  Poids\n",
      "\n",
      "*Group <  62.5\n",
      "   id Cheveux Taille  Poids Vegetarien class\n",
      "0   8      bl      p     40          o     N\n",
      "1   1      bl      m     45          n     N\n",
      "2   3      br      p     55          o     N\n",
      "3   2      bl      g     60          o     N\n",
      "\n",
      "Numeric column 'Poids':\n",
      "42.5\n",
      "[[1], [3]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "50.0\n",
      "[[2], [2]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "57.5\n",
      "[[3], [1]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "42.5\n",
      "0.0\n",
      "\n",
      "Non-numeric column 'Cheveux':\n",
      "  'bl ==> '0.0'\n",
      "  'br ==> '0.0'\n",
      "column'Cheveux' ==> entropy : '0.0'\n",
      "\n",
      "Non-numeric column 'Taille':\n",
      "  'p ==> '0.0'\n",
      "  'm ==> '0.0'\n",
      "  'g ==> '0.0'\n",
      "column'Taille' ==> entropy : '0.0'\n",
      "\n",
      "Non-numeric column 'Vegetarien':\n",
      "  'o ==> '0.0'\n",
      "  'n ==> '0.0'\n",
      "column'Vegetarien' ==> entropy : '0.0'\n",
      "******No more split\n",
      "\n",
      "*Group >=  62.5\n",
      "   id Cheveux Taille  Poids Vegetarien class\n",
      "4   4      bl      p     65          n     A\n",
      "5   5       r      m     75          n     A\n",
      "6   6      br      g     80          n     A\n",
      "7   7      br      m     85          n     A\n",
      "\n",
      "Numeric column 'Poids':\n",
      "70.0\n",
      "[[1], [3]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "77.5\n",
      "[[2], [2]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "82.5\n",
      "[[3], [1]]\n",
      "**** 0.0\n",
      "**** 0.0\n",
      "entropy =====>  0.0\n",
      "70.0\n",
      "0.0\n",
      "\n",
      "Non-numeric column 'Cheveux':\n",
      "  'bl ==> '0.0'\n",
      "  'r ==> '0.0'\n",
      "  'br ==> '0.0'\n",
      "column'Cheveux' ==> entropy : '0.0'\n",
      "\n",
      "Non-numeric column 'Taille':\n",
      "  'p ==> '0.0'\n",
      "  'm ==> '0.0'\n",
      "  'g ==> '0.0'\n",
      "column'Taille' ==> entropy : '0.0'\n",
      "\n",
      "Non-numeric column 'Vegetarien':\n",
      "  'n ==> '0.0'\n",
      "column'Vegetarien' ==> entropy : '0.0'\n",
      "******No more split\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "id3(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
