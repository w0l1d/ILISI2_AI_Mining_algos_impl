{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def entropy(values):\n",
    "    result = 0.0\n",
    "    div = sum(values)\n",
    "    for v in values:\n",
    "        if v > 0:\n",
    "            result -= v/div * math.log(v/div, 2)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def entropyTotale(values):\n",
    "    result = 0.0\n",
    "    total = 0\n",
    "    for e in values:\n",
    "        total += sum(e)\n",
    "    for e in values:\n",
    "        card = sum(e)\n",
    "        result += entropy(e) * card / total\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def entropyContinu(values):\n",
    "    result = 0\n",
    "    total = 0\n",
    "    for e in values:\n",
    "        total += sum(e)\n",
    "    for e in values:\n",
    "        result += entropy(e) * sum(e) / total\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def frequency_by_bins(df: pd.DataFrame, first_col: str, second_col: str):\n",
    "    # Compute the bin edges as the average of each pair of consecutive values in the first column\n",
    "    bin_edges = (df[first_col] + df[first_col].shift()) / 2\n",
    "\n",
    "    # Drop the first row since it will have a NaN value for the bin edge\n",
    "    bin_edges = bin_edges.dropna()\n",
    "\n",
    "    # Initialize empty dictionaries to store the frequencies before and after each bin edge\n",
    "    freqs_before = {}\n",
    "    freqs_after = {}\n",
    "\n",
    "    min_entropy = 1000\n",
    "    min_avg = 0\n",
    "    min_info = []\n",
    "    # Iterate over the bin edges and compute the frequencies before and after each one\n",
    "    for edge in bin_edges:\n",
    "        glob_list = []\n",
    "        sub_list1 = []\n",
    "        sun_list2 = []\n",
    "        print(edge)\n",
    "        # Find the index of the current bin edge in the original DataFrame\n",
    "        idx = df[first_col].searchsorted(edge)\n",
    "        #print(\"Before ==> \")\n",
    "        for v in df.iloc[:idx][second_col].value_counts():\n",
    "            #print(\"  \", v)\n",
    "            sub_list1.append(v)\n",
    "        glob_list.append(sub_list1)\n",
    "        #print(\"After ==> \")\n",
    "        for v in df.iloc[idx:][second_col].value_counts():\n",
    "            #print(\"  \", v)\n",
    "            sun_list2.append(v)\n",
    "        glob_list.append(sun_list2)\n",
    "        print(glob_list)\n",
    "\n",
    "        print(\"entropy =====> \", entropyContinu(glob_list))\n",
    "        if entropyContinu(glob_list) < min_entropy:\n",
    "            min_entropy = entropyContinu(glob_list)\n",
    "            min_avg = edge\n",
    "\n",
    "        # Compute the frequencies of the second column before and after the current bin edge\n",
    "        freqs_before[edge] = df.iloc[:idx][second_col].value_counts()\n",
    "        freqs_after[edge] = df.iloc[idx:][second_col].value_counts()\n",
    "\n",
    "        # Combine the dictionaries into a single DataFrame and return it\n",
    "    #freqs_df = pd.concat(\n",
    "        #[pd.concat([freqs_before[k], freqs_after[k]], axis=1, keys=['before', 'after'])\n",
    "         #for k in freqs_before.keys()],\n",
    "        #axis=1\n",
    "    #)\n",
    "    #freqs_df.columns.names = ['bin', 'value']\n",
    "    print(min_avg)\n",
    "    print(min_entropy)\n",
    "    min_info.append(min_entropy)\n",
    "    min_info.append(min_avg)\n",
    "    min_info.append(first_col)\n",
    "\n",
    "    #df1 = df[df[first_col] < 57.5]\n",
    "    #df2 = df[df[first_col] >= 57.5]\n",
    "\n",
    "    # Print the resulting groups\n",
    "    #print('Group 1:\\n', df1)\n",
    "    #print('Group 2:\\n', df2)\n",
    "\n",
    "    return min_info\n",
    "\n",
    "    #print(freqs_df)\n",
    "\n",
    "    #return groups"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data.csv')\n",
    "#frequency_by_bins(data, 'Poids', 'class')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def group_by_column(df: pd.DataFrame, column_to_group: str):\n",
    "    groups = df.drop(column_to_group, axis=1).groupby(df[column_to_group])\n",
    "    for group_name, group_data in groups:\n",
    "        print(\"\\n*Group:\", group_name)\n",
    "        print(group_data)\n",
    "        id3(group_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def id3(df: pd.DataFrame, test_date: pd.DataFrame = None, ignore_cols:list=[]):\n",
    "    ignore_cols.append('class')\n",
    "    ignore_cols.append('id')\n",
    "    class_col = 'class'\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ignore_cols]\n",
    "\n",
    "    cont_attr = []\n",
    "    for col in numeric_cols:\n",
    "        #counts = df.groupby(class_col)[col].value_counts()\n",
    "        #print(f\"Value counts for column '{col}' by {class_col}:\\n{counts}\\n\")\n",
    "        print(f\"\\nNumeric column '{col}':\")\n",
    "        cont_attr = frequency_by_bins(df, col, class_col)\n",
    "    if not cont_attr:\n",
    "        cont_attr = [1000]\n",
    "\n",
    "    non_numeric_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    non_numeric_cols = [col for col in non_numeric_cols if col not in ignore_cols]\n",
    "\n",
    "    non_numeric_counts = {}\n",
    "    for col in non_numeric_cols:\n",
    "        counts = df.groupby(class_col)[col].value_counts()\n",
    "        non_numeric_counts[col] = counts\n",
    "        #print(f\"Value counts for column '{col}' by {class_col}:\\n{counts}\\n\")\n",
    "\n",
    "    non_numeric_entropys = {}\n",
    "    for col, counts in non_numeric_counts.items():\n",
    "        print(f\"\\nNon-numeric column '{col}':\")\n",
    "        entropy_list = []\n",
    "        for col_value in df[col].unique():\n",
    "            group_counts = df.groupby(col)[class_col].value_counts()\n",
    "            #print(\"*****\", group_counts)\n",
    "            try:\n",
    "                attr_list = []\n",
    "                #print(\"**********\", col_value)\n",
    "                value_counts = group_counts[col_value]\n",
    "                for v in value_counts.to_dict().values():\n",
    "                    #print(v)\n",
    "                    attr_list.append(v)\n",
    "                entropy_list.append(attr_list)\n",
    "                print(f\"  '{col_value} ==> '{entropy(attr_list)}'\")\n",
    "                #print(attr_list)\n",
    "                #print(f\"Value '{col_value}': {value_counts.to_dict()}\")\n",
    "            except KeyError:\n",
    "                print(f\"Value '{col_value}': {0}\")\n",
    "        #print(entropy_list)\n",
    "        non_numeric_entropys[col] = entropyTotale(entropy_list)\n",
    "        print(f\"column'{col}' ==> entropy : '{non_numeric_entropys[col]}'\")\n",
    "    #print(min(non_numeric_entropys, key=non_numeric_entropys.get))\n",
    "    #print(min(non_numeric_entropys.values()))\n",
    "    min_non_numeric = 1000\n",
    "    if sum(non_numeric_entropys.values()) == 0.0 and (cont_attr[0] == 0.0 or cont_attr[0] == 1000):\n",
    "        print(\"******No more split\")\n",
    "    else:\n",
    "        if sum(non_numeric_entropys.values()) > 0.0:\n",
    "            min_non_numeric = min(non_numeric_entropys.values())\n",
    "        if min_non_numeric < cont_attr[0]:\n",
    "            #print(cont_attr[0])\n",
    "            print(\"===========> split by : \", min(non_numeric_entropys, key=non_numeric_entropys.get))\n",
    "            group_by_column(df, min(non_numeric_entropys, key=non_numeric_entropys.get))\n",
    "        else:\n",
    "            print(\"===========> split by : \", cont_attr[2])\n",
    "            df1 = df[df[cont_attr[2]] < cont_attr[1]]\n",
    "            df2 = df[df[cont_attr[2]] >= cont_attr[1]]\n",
    "            print(\"\\n*Group < \", cont_attr[1])\n",
    "            id3(df1)\n",
    "            print(\"\\n*Group >= \", cont_attr[1])\n",
    "            id3(df2)\n",
    "        #group_by_column(df, min(non_numeric_entropys, key=non_numeric_entropys.get))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric column 'poids':\n",
      "52.5\n",
      "[[1], [5, 2]]\n",
      "entropy =====>  0.7552304974958022\n",
      "57.5\n",
      "[[2, 1], [3, 2]]\n",
      "entropy =====>  0.9512050593046015\n",
      "60.0\n",
      "[[2, 1], [3, 2]]\n",
      "entropy =====>  0.9512050593046015\n",
      "70.0\n",
      "[[2, 2], [3, 1]]\n",
      "entropy =====>  0.9056390622295665\n",
      "77.5\n",
      "[[3, 2], [3]]\n",
      "entropy =====>  0.6068441215341679\n",
      "82.5\n",
      "[[3, 3], [2]]\n",
      "entropy =====>  0.75\n",
      "62.5\n",
      "[[2, 1], [3, 2]]\n",
      "entropy =====>  0.9512050593046015\n",
      "77.5\n",
      "0.6068441215341679\n",
      "\n",
      "Numeric column 'taile':\n",
      "2.5\n",
      "[[5, 3], []]\n",
      "entropy =====>  0.9544340029249649\n",
      "2.0\n",
      "[[2, 2], [3, 1]]\n",
      "entropy =====>  0.9056390622295665\n",
      "1.0\n",
      "[[], [5, 3]]\n",
      "entropy =====>  0.9544340029249649\n",
      "1.5\n",
      "[[2, 2], [3, 1]]\n",
      "entropy =====>  0.9056390622295665\n",
      "2.5\n",
      "[[5, 3], []]\n",
      "entropy =====>  0.9544340029249649\n",
      "2.5\n",
      "[[5, 3], []]\n",
      "entropy =====>  0.9544340029249649\n",
      "1.5\n",
      "[[2, 2], [3, 1]]\n",
      "entropy =====>  0.9056390622295665\n",
      "2.0\n",
      "0.9056390622295665\n",
      "\n",
      "Numeric column 'vegetarien':\n",
      "0.5\n",
      "[[4, 3], [1]]\n",
      "entropy =====>  0.8620746190299702\n",
      "1.0\n",
      "[[4, 3], [1]]\n",
      "entropy =====>  0.8620746190299702\n",
      "0.5\n",
      "[[4, 3], [1]]\n",
      "entropy =====>  0.8620746190299702\n",
      "0.0\n",
      "[[], [5, 3]]\n",
      "entropy =====>  0.9544340029249649\n",
      "0.0\n",
      "[[], [5, 3]]\n",
      "entropy =====>  0.9544340029249649\n",
      "0.0\n",
      "[[], [5, 3]]\n",
      "entropy =====>  0.9544340029249649\n",
      "0.5\n",
      "[[4, 3], [1]]\n",
      "entropy =====>  0.8620746190299702\n",
      "0.5\n",
      "0.8620746190299702\n",
      "\n",
      "Non-numeric column 'cheveux':\n",
      "  'blond ==> '1.0'\n",
      "  'brun ==> '0.0'\n",
      "  'rousse ==> '0.0'\n",
      "column'cheveux' ==> entropy : '0.5'\n",
      "===========> split by :  cheveux\n",
      "\n",
      "*Group: blond\n",
      "      id  poids  taile  vegetarien  class\n",
      "0  Sarah     45      2           0      1\n",
      "1   Dana     60      3           1      0\n",
      "3  Annie     65      1           0      1\n",
      "7  Katie     40      1           1      0\n",
      "\n",
      "Numeric column 'poids':\n",
      "52.5\n",
      "[[1], [2, 1]]\n",
      "entropy =====>  0.6887218755408672\n",
      "62.5\n",
      "[[1, 1], [1, 1]]\n",
      "entropy =====>  1.0\n",
      "52.5\n",
      "[[1], [2, 1]]\n",
      "entropy =====>  0.6887218755408672\n",
      "52.5\n",
      "0.6887218755408672\n",
      "\n",
      "Numeric column 'taile':\n",
      "2.5\n",
      "[[2, 2], []]\n",
      "entropy =====>  1.0\n",
      "2.0\n",
      "[[2, 2], []]\n",
      "entropy =====>  1.0\n",
      "1.0\n",
      "[[], [2, 2]]\n",
      "entropy =====>  1.0\n",
      "2.5\n",
      "1.0\n",
      "\n",
      "Numeric column 'vegetarien':\n",
      "0.5\n",
      "[[2, 1], [1]]\n",
      "entropy =====>  0.6887218755408672\n",
      "0.5\n",
      "[[2, 1], [1]]\n",
      "entropy =====>  0.6887218755408672\n",
      "0.5\n",
      "[[2, 1], [1]]\n",
      "entropy =====>  0.6887218755408672\n",
      "0.5\n",
      "0.6887218755408672\n",
      "===========> split by :  vegetarien\n",
      "\n",
      "*Group <  0.5\n",
      "\n",
      "Numeric column 'poids':\n",
      "55.0\n",
      "[[1], [1]]\n",
      "entropy =====>  0.0\n",
      "55.0\n",
      "0.0\n",
      "\n",
      "Numeric column 'taile':\n",
      "1.5\n",
      "[[2], []]\n",
      "entropy =====>  0.0\n",
      "1.5\n",
      "0.0\n",
      "\n",
      "Numeric column 'vegetarien':\n",
      "0.0\n",
      "[[], [2]]\n",
      "entropy =====>  0.0\n",
      "0.0\n",
      "0.0\n",
      "******No more split\n",
      "\n",
      "*Group >=  0.5\n",
      "\n",
      "Numeric column 'poids':\n",
      "50.0\n",
      "[[2], []]\n",
      "entropy =====>  0.0\n",
      "50.0\n",
      "0.0\n",
      "\n",
      "Numeric column 'taile':\n",
      "2.0\n",
      "[[2], []]\n",
      "entropy =====>  0.0\n",
      "2.0\n",
      "0.0\n",
      "\n",
      "Numeric column 'vegetarien':\n",
      "1.0\n",
      "[[], [2]]\n",
      "entropy =====>  0.0\n",
      "1.0\n",
      "0.0\n",
      "******No more split\n",
      "\n",
      "*Group: brun\n",
      "     id  poids  taile  vegetarien  class\n",
      "2  Alex     55      1           1      0\n",
      "5   Ali     80      3           0      0\n",
      "6  John     85      2           0      0\n",
      "\n",
      "Numeric column 'poids':\n",
      "67.5\n",
      "[[1], [2]]\n",
      "entropy =====>  0.0\n",
      "82.5\n",
      "[[2], [1]]\n",
      "entropy =====>  0.0\n",
      "67.5\n",
      "0.0\n",
      "\n",
      "Numeric column 'taile':\n",
      "2.0\n",
      "[[1], [2]]\n",
      "entropy =====>  0.0\n",
      "2.5\n",
      "[[1], [2]]\n",
      "entropy =====>  0.0\n",
      "2.0\n",
      "0.0\n",
      "\n",
      "Numeric column 'vegetarien':\n",
      "0.5\n",
      "[[3], []]\n",
      "entropy =====>  0.0\n",
      "0.0\n",
      "[[], [3]]\n",
      "entropy =====>  0.0\n",
      "0.5\n",
      "0.0\n",
      "******No more split\n",
      "\n",
      "*Group: rousse\n",
      "      id  poids  taile  vegetarien  class\n",
      "4  Emily     75      2           0      1\n",
      "\n",
      "Numeric column 'poids':\n",
      "0\n",
      "1000\n",
      "\n",
      "Numeric column 'taile':\n",
      "0\n",
      "1000\n",
      "\n",
      "Numeric column 'vegetarien':\n",
      "0\n",
      "1000\n",
      "******No more split\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "id3(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
